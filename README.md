# Projeto de Big Data: An√°lise Preditiva de Churn de Clientes

## üìñ Vis√£o Geral

[cite_start]Este reposit√≥rio cont√©m o projeto final desenvolvido para a disciplina de **Big Data e Minera√ß√£o de Dados**[cite: 949], como parte da Resid√™ncia em Intelig√™ncia Artificial Aplicada do UniSENAI. O objetivo foi construir uma solu√ß√£o de dados de ponta a ponta para prever a probabilidade de *churn* (cancelamento) de clientes, aplicando conceitos de an√°lise, engenharia de dados e machine learning.

[cite_start]O trabalho foi desenvolvido em dupla por Renan Mocelin e Lucas Porfirio[cite: 948].

## üéØ Objetivo do Projeto

[cite_start]O objetivo central foi elaborar e implementar um pipeline de dados e um modelo de minera√ß√£o para prever a probabilidade de churn de clientes[cite: 151]. [cite_start]A solu√ß√£o visa analisar dados hist√≥ricos para identificar os principais fatores de risco e gerar um "score de risco" acion√°vel, permitindo que as √°reas de neg√≥cio desenvolvam campanhas de reten√ß√£o mais eficazes[cite: 153].

## üìä Dataset

Utilizamos o dataset **"Telco Customer Churn"**, um conjunto de dados p√∫blico e popular para problemas de classifica√ß√£o, dispon√≠vel no Kaggle.

* **Link para o Dataset:** [https://www.kaggle.com/datasets/blastchar/telco-customer-churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

## üõ†Ô∏è Etapas do Projeto

O projeto foi estruturado em um fluxo de trabalho completo, simulando um case real da ind√∫stria:

1.  [cite_start]**An√°lise Explorat√≥ria de Dados (EDA):** Investiga√ß√£o inicial dos dados para identificar padr√µes e gerar hip√≥teses[cite: 167]. [cite_start]Os principais insights revelaram que o **tipo de contrato**, o **tempo de perman√™ncia (tenure)** e o **valor da cobran√ßa mensal** s√£o os fatores mais influentes no churn[cite: 298, 874].

2.  [cite_start]**Engenharia de Atributos e Pr√©-processamento:** Transforma√ß√£o das vari√°veis categ√≥ricas em formato num√©rico (`One-Hot Encoding`) e padroniza√ß√£o das vari√°veis cont√≠nuas (`StandardScaler`) para preparar os dados para a modelagem[cite: 365, 367].

3.  [cite_start]**Sele√ß√£o de Vari√°veis (Feature Selection):** Aplica√ß√£o de um modelo Random Forest sobre todos os atributos para identificar, de forma orientada por dados, as vari√°veis com maior poder preditivo[cite: 371].

4.  [cite_start]**Modelagem Preditiva:** Treinamento e avalia√ß√£o de dois modelos de classifica√ß√£o (Regress√£o Log√≠stica e Random Forest)[cite: 379]. [cite_start]O modelo de Regress√£o Log√≠stica foi selecionado como a solu√ß√£o final por sua performance robusta e maior interpretabilidade[cite: 386].

5.  [cite_start]**Arquitetura do Pipeline de Dados:** Desenho de uma arquitetura de pipeline escal√°vel na nuvem **AWS**, utilizando servi√ßos como S3 (Data Lake), AWS Glue (ETL), Amazon SageMaker (Modelagem) e QuickSight (BI) [cite: 325-339, 911, 916, 923].

## üöÄ Tecnologias Utilizadas

* **Linguagem:** Python 3
* **Bibliotecas Principais:** Pandas, Scikit-learn, Matplotlib, Seaborn
* **Ambiente:** Google Colab
* **Cloud (Arquitetura):** Amazon Web Services (AWS)

## ‚öôÔ∏è Como Executar o Notebook

[cite_start]O projeto est√° contido no notebook `BigData (1).ipynb` [cite: 1-142]. Para execut√°-lo:

1.  Abra o notebook no Google Colab.
2.  Para a etapa de obten√ß√£o de dados via API, ser√° necess√°rio gerar sua pr√≥pria chave `kaggle.json` no site do Kaggle e fazer o upload quando o script solicitar.
3.  Execute as c√©lulas em ordem sequencial.

---
