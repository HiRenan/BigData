# Projeto de Big Data: An√°lise Preditiva de Churn de Clientes

## üìñ Vis√£o Geral

Este reposit√≥rio cont√©m o projeto final desenvolvido para a disciplina de **Big Data e Minera√ß√£o de Dados**, como parte da Resid√™ncia em Intelig√™ncia Artificial Aplicada do UniSENAI. O objetivo foi construir uma solu√ß√£o de dados de ponta a ponta para prever a probabilidade de *churn* (cancelamento) de clientes, aplicando conceitos de an√°lise, engenharia de dados e machine learning.

O trabalho foi desenvolvido em dupla por Renan Mocelin e Lucas Porfirio.

## üéØ Objetivo do Projeto

O objetivo central foi elaborar e implementar um pipeline de dados e um modelo de minera√ß√£o para prever a probabilidade de churn de clientes. A solu√ß√£o visa analisar dados hist√≥ricos para identificar os principais fatores de risco e gerar um "score de risco" acion√°vel, permitindo que as √°reas de neg√≥cio desenvolvam campanhas de reten√ß√£o mais eficazes.

## üìä Dataset

Utilizamos o dataset **"Telco Customer Churn"**, um conjunto de dados p√∫blico e popular para problemas de classifica√ß√£o, dispon√≠vel no Kaggle.

* **Link para o Dataset:** [https://www.kaggle.com/datasets/blastchar/telco-customer-churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

## üõ†Ô∏è Etapas do Projeto

O projeto foi estruturado em um fluxo de trabalho completo, simulando um case real da ind√∫stria:

1.  **An√°lise Explorat√≥ria de Dados (EDA):** Investiga√ß√£o inicial dos dados para identificar padr√µes e gerar hip√≥teses. Os principais insights revelaram que o **tipo de contrato**, o **tempo de perman√™ncia (tenure)** e o **valor da cobran√ßa mensal** s√£o os fatores mais influentes no churn.

2.  **Engenharia de Atributos e Pr√©-processamento:** Transforma√ß√£o das vari√°veis categ√≥ricas em formato num√©rico (`One-Hot Encoding`) e padroniza√ß√£o das vari√°veis cont√≠nuas (`StandardScaler`) para preparar os dados para a modelagem.

3.  **Sele√ß√£o de Vari√°veis (Feature Selection):** Aplica√ß√£o de um modelo Random Forest sobre todos os atributos para identificar, de forma orientada por dados, as vari√°veis com maior poder preditivo.

4.  **Modelagem Preditiva:** Treinamento e avalia√ß√£o de dois modelos de classifica√ß√£o (Regress√£o Log√≠stica e Random Forest). O modelo de Regress√£o Log√≠stica foi selecionado como a solu√ß√£o final por sua performance robusta e maior interpretabilidade.

5.  **Arquitetura do Pipeline de Dados:** Desenho de uma arquitetura de pipeline escal√°vel na nuvem **AWS**, utilizando servi√ßos como S3 (Data Lake), AWS Glue (ETL), Amazon SageMaker (Modelagem) e QuickSight (BI).

## üöÄ Tecnologias Utilizadas

* **Linguagem:** Python 3
* **Bibliotecas Principais:** Pandas, Scikit-learn, Matplotlib, Seaborn
* **Ambiente:** Google Colab
* **Cloud (Arquitetura):** Amazon Web Services (AWS)

## ‚öôÔ∏è Como Executar o Notebook

O projeto est√° contido no notebook `BigData (1).ipynb` . Para execut√°-lo:

1.  Abra o notebook no Google Colab.
2.  Para a etapa de obten√ß√£o de dados via API, ser√° necess√°rio gerar sua pr√≥pria chave `kaggle.json` no site do Kaggle e fazer o upload quando o script solicitar.
3.  Execute as c√©lulas em ordem sequencial.

---
